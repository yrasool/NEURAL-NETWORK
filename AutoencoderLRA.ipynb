{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yrasool/NEURAL-NETWORK/blob/main/AutoencoderLRA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import numpy as np\n",
        "\n",
        "# Global parameters\n",
        "minibatch_size = 50\n",
        "learning_rate = 0.001\n",
        "beta = 0.1\n",
        "## model 1\n",
        "size_input = 784 # MNIST data input (img shape: 28*28)\n",
        "size_hidden = 64\n",
        "\n",
        "size_output = 10\n",
        "gamma = 1.0\n",
        "\n",
        "class Encoder:\n",
        "    def __init__(self, input_size, encoding_dim):\n",
        "        # Define sizes\n",
        "        self.size_input = input_size\n",
        "        self.size_hidden = 64\n",
        "        self.size_output = encoding_dim\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.w1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden], stddev=0.1), trainable=True, name=\"W1\")\n",
        "        self.b1 = tf.Variable(tf.zeros([self.size_hidden]), name='b1')\n",
        "        self.w2 = tf.Variable(tf.random.normal([self.size_hidden, self.size_output], stddev=0.1), trainable=True, name='W2')\n",
        "        self.b2 = tf.Variable(tf.zeros([self.size_output]), name='encoder_b2')\n",
        "\n",
        "        # Initialize feedback weights\n",
        "        self.E2 = tf.Variable(tf.random.normal([self.size_hidden, self.size_hidden], stddev=1.0), name=\"E2\")\n",
        "        self.E3 = tf.Variable(tf.random.normal([self.size_hidden, self.size_hidden], stddev=1.0), name=\"E3\")\n",
        "\n",
        "    def compute_output(self, x):\n",
        "        #print(f\"w1:, {self.w1.shape}\")\n",
        "        #print(f\"b1:,{self.b1.shape}\")\n",
        "        z1 = tf.matmul(x, self.w1) + self.b1\n",
        "        a1 = tf.nn.relu(z1)\n",
        "        #print(\"[forward] a1 shape (after ReLU):\", a1.shape)\n",
        "\n",
        "        #print(f\"w2:, {self.w2.shape}\")\n",
        "        #print(f\"b2:,{self.b2.shape}\")\n",
        "        z2 = tf.matmul(a1, self.w2) + self.b2\n",
        "        #print(\"[forward] z2 shape:\", z2.shape)\n",
        "        a2 = tf.nn.relu(z2)\n",
        "\n",
        "        return {\n",
        "            'layer1_pre_activation': z1,\n",
        "            'layer1_activation': a1,\n",
        "            'layer2_pre_activation': z2,\n",
        "            'layer2_activation': a2,\n",
        "            'final_encoding': a2\n",
        "        }\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.compute_output(x)['final_encoding']\n",
        "\n",
        "    @property\n",
        "    def variables(self):\n",
        "        return [self.w1,  self.w2,  self.E2, self.E3]\n",
        "\n",
        "class Decoder:\n",
        "    def __init__(self, encoding_dim, output_size):\n",
        "        # Define sizes\n",
        "        self.size_input = encoding_dim\n",
        "        self.size_hidden = 64\n",
        "        self.size_output = output_size\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.w3 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden], stddev=0.1), trainable=True, name=\"W3\")\n",
        "        self.b3 = tf.Variable(tf.zeros([self.size_hidden]), name='b3')\n",
        "        self.w4 = tf.Variable(tf.random.normal([self.size_hidden, self.size_output], stddev=0.1), trainable=True, name='W4')\n",
        "        self.b4 = tf.Variable(tf.zeros([self.size_output]), name='decoder_b4')\n",
        "\n",
        "        self.E4 = tf.Variable(tf.random.normal([self.size_output, self.size_hidden], stddev=1.0), name=\"E4\")\n",
        "\n",
        "    def compute_output(self, encoded):\n",
        "        #print(f\"w3: {self.w3.shape}\")\n",
        "        #print(f\"b3:,{self.b3.shape}\")\n",
        "        z3 = tf.matmul(encoded, self.w3) + self.b3\n",
        "        #print(\"[forward] z3 shape:\", z3.shape)\n",
        "        a3 = tf.nn.relu(z3)\n",
        "        #print(f\"w4:, {self.w4.shape}\")\n",
        "        #print(f\"b4:,{self.b4.shape}\")\n",
        "        z4 = tf.matmul(a3, self.w4) + self.b4\n",
        "        #print(\"[forward] z4 shape:\", z4.shape)\n",
        "        reconstruction = tf.nn.sigmoid(z4)\n",
        "\n",
        "        return {\n",
        "            'layer1_pre_activation': z3,\n",
        "            'layer1_activation': a3,\n",
        "            'layer2_pre_activation': z4,\n",
        "            'final_reconstruction': reconstruction\n",
        "        }\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.compute_output(x)['final_reconstruction']\n",
        "\n",
        "    @property\n",
        "    def variables(self):\n",
        "        return [self.w3, self.w4, self.E4]\n",
        "\n",
        "class Autoencoder:\n",
        "    def __init__(self, input_size, encoding_dim):\n",
        "        self.encoder = Encoder(input_size, encoding_dim)\n",
        "        self.decoder = Decoder(encoding_dim, input_size)\n",
        "        self.optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "        # Initialize storage for intermediate values\n",
        "        self.z1 = None\n",
        "        self.a1 = None\n",
        "        self.z2 = None\n",
        "        self.a2 = None\n",
        "        self.z3 = None\n",
        "        self.a3 = None\n",
        "        self.decoder_outputs = None\n",
        "        self.hhat1 = None  # Will store target representations\n",
        "\n",
        "    def encode_and_decode(self, x):\n",
        "        # Forward pass\n",
        "        encoder_outputs = self.encoder.compute_output(x)\n",
        "        decoder_outputs = self.decoder.compute_output(encoder_outputs['final_encoding'])\n",
        "\n",
        "        # Store activations for LRA updates\n",
        "        self.z1 = encoder_outputs['layer1_pre_activation']\n",
        "        self.a1 = encoder_outputs['layer1_activation']\n",
        "        self.z2 = encoder_outputs['layer2_pre_activation']\n",
        "        self.a2 = encoder_outputs['layer2_activation']\n",
        "        self.z3 = decoder_outputs['layer1_pre_activation']\n",
        "        self.a3 = decoder_outputs['layer1_activation']\n",
        "        self.decoder_outputs = decoder_outputs['final_reconstruction']\n",
        "\n",
        "        return {\n",
        "            'encoder_outputs': encoder_outputs,\n",
        "            'decoder_outputs': decoder_outputs,\n",
        "            'final_reconstruction': decoder_outputs['final_reconstruction']\n",
        "        }\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.encode_and_decode(x)['final_reconstruction']\n",
        "\n",
        "    @property\n",
        "    def variables(self):\n",
        "       return [\n",
        "        self.encoder.w1,\n",
        "        self.encoder.w2,\n",
        "        self.decoder.w3,\n",
        "        self.decoder.w4,\n",
        "        self.encoder.E2,\n",
        "        self.encoder.E3,\n",
        "        self.decoder.E4\n",
        "      ]\n",
        "\n",
        "\n",
        "    def compute_loss(self, x_train):\n",
        "        reconstruction_error = tf.subtract(self.decoder_outputs, x_train)\n",
        "        return tf.reduce_mean(tf.square(reconstruction_error))\n",
        "\n",
        "    def compute_lraupdates(self, x_train):\n",
        "        # Forward pass first\n",
        "        self.encode_and_decode(x_train)\n",
        "\n",
        "        # Compute errors and updates\n",
        "        e4 = tf.subtract(self.decoder_outputs, x_train)  # (50,784)\n",
        "        e4_T = tf.transpose(e4)                          # (784,50)\n",
        "\n",
        "        # Compute d3\n",
        "        d3 = tf.matmul(tf.transpose(self.decoder.E4), e4_T)  # E4: (784,64) → E4^T: (64,784) @ e4_T: (784,50) → d3: (64,50)\n",
        "        d3_b = tf.multiply(d3, beta)                        # (64,50)\n",
        "\n",
        "        # Compute y3_z\n",
        "        z3_T = tf.transpose(self.z3)                         # z3: (50,64) → z3_T: (64,50)\n",
        "        y3_z = tf.nn.tanh(tf.subtract(z3_T, d3_b))           # (64,50)\n",
        "\n",
        "        # Compute e3\n",
        "        e3 = tf.subtract(self.a2, tf.transpose(y3_z))        # a2: (50,64) - y3_z_T: (50,64) → e3: (50,64)\n",
        "\n",
        "        # Compute d2\n",
        "        e3_T = tf.transpose(e3)\n",
        "        #print(e3_T.shape)\n",
        "        #print(self.encoder.E3.shape)\n",
        "        d2 = tf.matmul(self.encoder.E3, e3_T)\n",
        "        # E3: (64,64) @ e3_T: (64,50) → d2: (64,50)\n",
        "        d2_b = tf.multiply(d2, beta)                        # (64,50)\n",
        "\n",
        "        # Compute y2_z\n",
        "        z2_T = tf.transpose(self.z2)                         # z2: (50,64) → z2_T: (64,50)\n",
        "        y2_z = tf.nn.tanh(tf.subtract(z2_T, d2_b))           # (64,50)\n",
        "\n",
        "        # Compute e2\n",
        "        e2 = tf.subtract(self.a2, tf.transpose(y2_z))     # hhat1: (50,64) - y2_z_T: (50,64) → e2: (50,64)\n",
        "\n",
        "        # Compute d1\n",
        "        e2_T = tf.transpose(e2)\n",
        "        #print(e2.shape)# (64,50)\n",
        "        d1 = tf.matmul(self.encoder.E2, e2_T)                # E2: (64,64) @ e2_T: (64,50) → d1: (64,50)\n",
        "        d1_b = tf.multiply(d1, beta)                        # (64,50)\n",
        "\n",
        "        # Compute y1_z\n",
        "        z1_T = tf.transpose(self.z1)                         # z1: (50,64) → z1_T: (64,50)\n",
        "        y1_z = tf.nn.tanh(tf.subtract(z1_T, d1_b))           # (64,50)\n",
        "\n",
        "        # Compute e1\n",
        "        e1 = tf.subtract(self.z1, tf.transpose(y1_z))        # z1: (50,64) - y1_z_T: (50,64) → e1: (50,64)\n",
        "\n",
        "        # Compute weight updates\n",
        "        dW4 = tf.matmul(e4, self.z3, transpose_a=True)      # e4: (50,784) @ z3: (50,64) with transpose_a=True → (784,64)\n",
        "        dW3 = tf.matmul(e3, self.z2, transpose_a=True)      # e3: (50,64) @ z2: (50,64) with transpose_a=True → (64,64)\n",
        "        dW2 = tf.matmul(e2, self.z1, transpose_a=True)      # e2: (50,64) @ z1: (50,64) with transpose_a=True → (64,64)\n",
        "        dW1 = tf.matmul(e1, x_train, transpose_a=True)      # e1: (50,64) @ x_train: (50,784) with transpose_a=True → (64,784)\n",
        "\n",
        "        # Compute eligible traces\n",
        "        dW4_e = tf.multiply(dW4, gamma)\n",
        "\n",
        "        # (784,64)\n",
        "        dW3_e = tf.multiply(dW3, gamma)\n",
        "\n",
        "        dW2_e = tf.multiply(dW2, gamma)\n",
        "\n",
        "\n",
        "        # Combine gradients\n",
        "        grads_w = [\n",
        "            tf.transpose(dW1),  # (784,64) → matches w1: (784,64)\n",
        "            tf.transpose(dW2),  # (64,64) → matches w2: (64,64)\n",
        "            tf.transpose(dW3),  # (64,64) → matches w3: (64,64)\n",
        "            tf.transpose(dW4),  # (64,784) → matches w4: (64,784)\n",
        "            dW2_e,              # (64,64) → matches E2: (64,64)\n",
        "            dW3_e,              # (64,64) → matches E3: (64,64)\n",
        "            dW4_e               # (784,64) → matches E4: (784,64)\n",
        "        ]\n",
        "        #for g, v in zip(grads_w, self.variables):\n",
        "           #print(f\"Variable {v.name} shape: {v.shape}, Gradient shape: {g.shape}\")\n",
        "\n",
        "\n",
        "        # Apply updates\n",
        "        self.optimizer.apply_gradients(zip(grads_w, self.variables))\n",
        "\n",
        "        # Return current loss\n",
        "        return self.compute_loss(x_train)\n",
        "def train_autoencoder():\n",
        "    # Prepare MNIST dataset\n",
        "    (x_train, _), (x_test, _) = mnist.load_data()\n",
        "    x_train = x_train.astype(np.float32) / 255.0\n",
        "    x_test = x_test.astype(np.float32) / 255.0\n",
        "\n",
        "    x_train = x_train.reshape(-1, 784)\n",
        "    x_test = x_test.reshape(-1, 784)\n",
        "\n",
        "    # Initialize autoencoder\n",
        "    input_size = 784  # 28*28\n",
        "    encoding_dim = 64\n",
        "    autoencoder = Autoencoder(input_size, encoding_dim)\n",
        "\n",
        "    # Training loop\n",
        "    epochs = 10\n",
        "    batch_size = minibatch_size\n",
        "    n_batches = len(x_train) // batch_size\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        # Shuffle training data\n",
        "        idx = np.random.permutation(len(x_train))\n",
        "        x_train_shuffled = x_train[idx]\n",
        "\n",
        "        for batch_idx in range(n_batches):\n",
        "            start_idx = batch_idx * batch_size\n",
        "            end_idx = start_idx + batch_size\n",
        "            batch = x_train_shuffled[start_idx:end_idx]\n",
        "\n",
        "            # Train on batch using LRA updates\n",
        "            loss = autoencoder.compute_lraupdates(batch)\n",
        "            total_loss += loss\n",
        "\n",
        "            #if batch_idx % 10 == 0:\n",
        "                #print(f\"Epoch {epoch+1}/{epochs}, Batch {batch_idx}/{n_batches}, Loss: {loss:.4f}\")\n",
        "\n",
        "        # Print epoch results\n",
        "        avg_loss = total_loss / n_batches\n",
        "        print(f\"Epoch {epoch+1} completed, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Evaluate on test set\n",
        "        test_reconstructions = autoencoder(x_test[:1000])\n",
        "        test_loss = tf.reduce_mean(tf.square(x_test[:1000] - test_reconstructions))\n",
        "        print(f\"Test Loss: {test_loss:.4f}\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_autoencoder()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oO4oynFKdrIL",
        "outputId": "13fbffa9-f12c-465e-d484-d0474f40003d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Batch 0/1200, Loss: 0.2312\n",
            "Epoch 1/10, Batch 10/1200, Loss: 0.2260\n",
            "Epoch 1/10, Batch 20/1200, Loss: 0.2214\n",
            "Epoch 1/10, Batch 30/1200, Loss: 0.2099\n",
            "Epoch 1/10, Batch 40/1200, Loss: 0.1942\n",
            "Epoch 1/10, Batch 50/1200, Loss: 0.1602\n",
            "Epoch 1/10, Batch 60/1200, Loss: 0.1184\n",
            "Epoch 1/10, Batch 70/1200, Loss: 0.0918\n",
            "Epoch 1/10, Batch 80/1200, Loss: 0.0757\n",
            "Epoch 1/10, Batch 90/1200, Loss: 0.0761\n",
            "Epoch 1/10, Batch 100/1200, Loss: 0.0750\n",
            "Epoch 1/10, Batch 110/1200, Loss: 0.0713\n",
            "Epoch 1/10, Batch 120/1200, Loss: 0.0741\n",
            "Epoch 1/10, Batch 130/1200, Loss: 0.0722\n",
            "Epoch 1/10, Batch 140/1200, Loss: 0.0681\n",
            "Epoch 1/10, Batch 150/1200, Loss: 0.0695\n",
            "Epoch 1/10, Batch 160/1200, Loss: 0.0731\n",
            "Epoch 1/10, Batch 170/1200, Loss: 0.0699\n",
            "Epoch 1/10, Batch 180/1200, Loss: 0.0709\n",
            "Epoch 1/10, Batch 190/1200, Loss: 0.0666\n",
            "Epoch 1/10, Batch 200/1200, Loss: 0.0709\n",
            "Epoch 1/10, Batch 210/1200, Loss: 0.0675\n",
            "Epoch 1/10, Batch 220/1200, Loss: 0.0714\n",
            "Epoch 1/10, Batch 230/1200, Loss: 0.0710\n",
            "Epoch 1/10, Batch 240/1200, Loss: 0.0719\n",
            "Epoch 1/10, Batch 250/1200, Loss: 0.0687\n",
            "Epoch 1/10, Batch 260/1200, Loss: 0.0655\n",
            "Epoch 1/10, Batch 270/1200, Loss: 0.0661\n",
            "Epoch 1/10, Batch 280/1200, Loss: 0.0697\n",
            "Epoch 1/10, Batch 290/1200, Loss: 0.0723\n",
            "Epoch 1/10, Batch 300/1200, Loss: 0.0736\n",
            "Epoch 1/10, Batch 310/1200, Loss: 0.0720\n",
            "Epoch 1/10, Batch 320/1200, Loss: 0.0711\n",
            "Epoch 1/10, Batch 330/1200, Loss: 0.0718\n",
            "Epoch 1/10, Batch 340/1200, Loss: 0.0655\n",
            "Epoch 1/10, Batch 350/1200, Loss: 0.0735\n",
            "Epoch 1/10, Batch 360/1200, Loss: 0.0664\n",
            "Epoch 1/10, Batch 370/1200, Loss: 0.0681\n",
            "Epoch 1/10, Batch 380/1200, Loss: 0.0777\n",
            "Epoch 1/10, Batch 390/1200, Loss: 0.0695\n",
            "Epoch 1/10, Batch 400/1200, Loss: 0.0740\n",
            "Epoch 1/10, Batch 410/1200, Loss: 0.0745\n",
            "Epoch 1/10, Batch 420/1200, Loss: 0.0696\n",
            "Epoch 1/10, Batch 430/1200, Loss: 0.0685\n",
            "Epoch 1/10, Batch 440/1200, Loss: 0.0696\n",
            "Epoch 1/10, Batch 450/1200, Loss: 0.0695\n",
            "Epoch 1/10, Batch 460/1200, Loss: 0.0720\n",
            "Epoch 1/10, Batch 470/1200, Loss: 0.0677\n",
            "Epoch 1/10, Batch 480/1200, Loss: 0.0704\n",
            "Epoch 1/10, Batch 490/1200, Loss: 0.0701\n",
            "Epoch 1/10, Batch 500/1200, Loss: 0.0702\n",
            "Epoch 1/10, Batch 510/1200, Loss: 0.0676\n",
            "Epoch 1/10, Batch 520/1200, Loss: 0.0718\n",
            "Epoch 1/10, Batch 530/1200, Loss: 0.0686\n",
            "Epoch 1/10, Batch 540/1200, Loss: 0.0688\n",
            "Epoch 1/10, Batch 550/1200, Loss: 0.0680\n",
            "Epoch 1/10, Batch 560/1200, Loss: 0.0674\n",
            "Epoch 1/10, Batch 570/1200, Loss: 0.0705\n",
            "Epoch 1/10, Batch 580/1200, Loss: 0.0705\n",
            "Epoch 1/10, Batch 590/1200, Loss: 0.0706\n",
            "Epoch 1/10, Batch 600/1200, Loss: 0.0712\n",
            "Epoch 1/10, Batch 610/1200, Loss: 0.0734\n",
            "Epoch 1/10, Batch 620/1200, Loss: 0.0727\n",
            "Epoch 1/10, Batch 630/1200, Loss: 0.0704\n",
            "Epoch 1/10, Batch 640/1200, Loss: 0.0683\n",
            "Epoch 1/10, Batch 650/1200, Loss: 0.0735\n",
            "Epoch 1/10, Batch 660/1200, Loss: 0.0698\n",
            "Epoch 1/10, Batch 670/1200, Loss: 0.0745\n",
            "Epoch 1/10, Batch 680/1200, Loss: 0.0698\n",
            "Epoch 1/10, Batch 690/1200, Loss: 0.0666\n",
            "Epoch 1/10, Batch 700/1200, Loss: 0.0750\n",
            "Epoch 1/10, Batch 710/1200, Loss: 0.0664\n",
            "Epoch 1/10, Batch 720/1200, Loss: 0.0714\n",
            "Epoch 1/10, Batch 730/1200, Loss: 0.0717\n",
            "Epoch 1/10, Batch 740/1200, Loss: 0.0650\n",
            "Epoch 1/10, Batch 750/1200, Loss: 0.0686\n",
            "Epoch 1/10, Batch 760/1200, Loss: 0.0720\n",
            "Epoch 1/10, Batch 770/1200, Loss: 0.0667\n",
            "Epoch 1/10, Batch 780/1200, Loss: 0.0714\n",
            "Epoch 1/10, Batch 790/1200, Loss: 0.0748\n",
            "Epoch 1/10, Batch 800/1200, Loss: 0.0737\n",
            "Epoch 1/10, Batch 810/1200, Loss: 0.0697\n",
            "Epoch 1/10, Batch 820/1200, Loss: 0.0696\n",
            "Epoch 1/10, Batch 830/1200, Loss: 0.0715\n",
            "Epoch 1/10, Batch 840/1200, Loss: 0.0666\n",
            "Epoch 1/10, Batch 850/1200, Loss: 0.0678\n",
            "Epoch 1/10, Batch 860/1200, Loss: 0.0730\n",
            "Epoch 1/10, Batch 870/1200, Loss: 0.0709\n",
            "Epoch 1/10, Batch 880/1200, Loss: 0.0710\n",
            "Epoch 1/10, Batch 890/1200, Loss: 0.0731\n",
            "Epoch 1/10, Batch 900/1200, Loss: 0.0726\n",
            "Epoch 1/10, Batch 910/1200, Loss: 0.0723\n",
            "Epoch 1/10, Batch 920/1200, Loss: 0.0715\n",
            "Epoch 1/10, Batch 930/1200, Loss: 0.0691\n",
            "Epoch 1/10, Batch 940/1200, Loss: 0.0708\n",
            "Epoch 1/10, Batch 950/1200, Loss: 0.0672\n",
            "Epoch 1/10, Batch 960/1200, Loss: 0.0733\n",
            "Epoch 1/10, Batch 970/1200, Loss: 0.0781\n",
            "Epoch 1/10, Batch 980/1200, Loss: 0.0728\n",
            "Epoch 1/10, Batch 990/1200, Loss: 0.0722\n",
            "Epoch 1/10, Batch 1000/1200, Loss: 0.0657\n",
            "Epoch 1/10, Batch 1010/1200, Loss: 0.0712\n",
            "Epoch 1/10, Batch 1020/1200, Loss: 0.0723\n",
            "Epoch 1/10, Batch 1030/1200, Loss: 0.0745\n",
            "Epoch 1/10, Batch 1040/1200, Loss: 0.0742\n",
            "Epoch 1/10, Batch 1050/1200, Loss: 0.0718\n",
            "Epoch 1/10, Batch 1060/1200, Loss: 0.0678\n",
            "Epoch 1/10, Batch 1070/1200, Loss: 0.0654\n",
            "Epoch 1/10, Batch 1080/1200, Loss: 0.0723\n",
            "Epoch 1/10, Batch 1090/1200, Loss: 0.0716\n",
            "Epoch 1/10, Batch 1100/1200, Loss: 0.0752\n",
            "Epoch 1/10, Batch 1110/1200, Loss: 0.0699\n",
            "Epoch 1/10, Batch 1120/1200, Loss: 0.0695\n",
            "Epoch 1/10, Batch 1130/1200, Loss: 0.0676\n",
            "Epoch 1/10, Batch 1140/1200, Loss: 0.0742\n",
            "Epoch 1/10, Batch 1150/1200, Loss: 0.0691\n",
            "Epoch 1/10, Batch 1160/1200, Loss: 0.0693\n",
            "Epoch 1/10, Batch 1170/1200, Loss: 0.0720\n",
            "Epoch 1/10, Batch 1180/1200, Loss: 0.0731\n",
            "Epoch 1/10, Batch 1190/1200, Loss: 0.0687\n",
            "Epoch 1 completed, Average Loss: 0.0775\n",
            "Test Loss: 0.0687\n",
            "\n",
            "Epoch 2/10, Batch 0/1200, Loss: 0.0737\n",
            "Epoch 2/10, Batch 10/1200, Loss: 0.0714\n",
            "Epoch 2/10, Batch 20/1200, Loss: 0.0684\n",
            "Epoch 2/10, Batch 30/1200, Loss: 0.0685\n",
            "Epoch 2/10, Batch 40/1200, Loss: 0.0725\n",
            "Epoch 2/10, Batch 50/1200, Loss: 0.0712\n",
            "Epoch 2/10, Batch 60/1200, Loss: 0.0707\n",
            "Epoch 2/10, Batch 70/1200, Loss: 0.0699\n",
            "Epoch 2/10, Batch 80/1200, Loss: 0.0718\n",
            "Epoch 2/10, Batch 90/1200, Loss: 0.0751\n",
            "Epoch 2/10, Batch 100/1200, Loss: 0.0749\n",
            "Epoch 2/10, Batch 110/1200, Loss: 0.0706\n",
            "Epoch 2/10, Batch 120/1200, Loss: 0.0709\n",
            "Epoch 2/10, Batch 130/1200, Loss: 0.0697\n",
            "Epoch 2/10, Batch 140/1200, Loss: 0.0686\n",
            "Epoch 2/10, Batch 150/1200, Loss: 0.0735\n",
            "Epoch 2/10, Batch 160/1200, Loss: 0.0697\n",
            "Epoch 2/10, Batch 170/1200, Loss: 0.0699\n",
            "Epoch 2/10, Batch 180/1200, Loss: 0.0713\n",
            "Epoch 2/10, Batch 190/1200, Loss: 0.0748\n",
            "Epoch 2/10, Batch 200/1200, Loss: 0.0763\n",
            "Epoch 2/10, Batch 210/1200, Loss: 0.0681\n",
            "Epoch 2/10, Batch 220/1200, Loss: 0.0742\n",
            "Epoch 2/10, Batch 230/1200, Loss: 0.0777\n",
            "Epoch 2/10, Batch 240/1200, Loss: 0.0710\n",
            "Epoch 2/10, Batch 250/1200, Loss: 0.0720\n",
            "Epoch 2/10, Batch 260/1200, Loss: 0.0689\n",
            "Epoch 2/10, Batch 270/1200, Loss: 0.0646\n",
            "Epoch 2/10, Batch 280/1200, Loss: 0.0663\n",
            "Epoch 2/10, Batch 290/1200, Loss: 0.0746\n",
            "Epoch 2/10, Batch 300/1200, Loss: 0.0711\n",
            "Epoch 2/10, Batch 310/1200, Loss: 0.0736\n",
            "Epoch 2/10, Batch 320/1200, Loss: 0.0692\n",
            "Epoch 2/10, Batch 330/1200, Loss: 0.0757\n",
            "Epoch 2/10, Batch 340/1200, Loss: 0.0724\n",
            "Epoch 2/10, Batch 350/1200, Loss: 0.0740\n",
            "Epoch 2/10, Batch 360/1200, Loss: 0.0734\n",
            "Epoch 2/10, Batch 370/1200, Loss: 0.0712\n",
            "Epoch 2/10, Batch 380/1200, Loss: 0.0787\n",
            "Epoch 2/10, Batch 390/1200, Loss: 0.0713\n",
            "Epoch 2/10, Batch 400/1200, Loss: 0.0740\n",
            "Epoch 2/10, Batch 410/1200, Loss: 0.0685\n",
            "Epoch 2/10, Batch 420/1200, Loss: 0.0725\n",
            "Epoch 2/10, Batch 430/1200, Loss: 0.0743\n",
            "Epoch 2/10, Batch 440/1200, Loss: 0.0654\n",
            "Epoch 2/10, Batch 450/1200, Loss: 0.0660\n",
            "Epoch 2/10, Batch 460/1200, Loss: 0.0723\n",
            "Epoch 2/10, Batch 470/1200, Loss: 0.0702\n",
            "Epoch 2/10, Batch 480/1200, Loss: 0.0778\n",
            "Epoch 2/10, Batch 490/1200, Loss: 0.0716\n",
            "Epoch 2/10, Batch 500/1200, Loss: 0.0729\n",
            "Epoch 2/10, Batch 510/1200, Loss: 0.0733\n",
            "Epoch 2/10, Batch 520/1200, Loss: 0.0759\n",
            "Epoch 2/10, Batch 530/1200, Loss: 0.0701\n",
            "Epoch 2/10, Batch 540/1200, Loss: 0.0726\n",
            "Epoch 2/10, Batch 550/1200, Loss: 0.0744\n",
            "Epoch 2/10, Batch 560/1200, Loss: 0.0730\n",
            "Epoch 2/10, Batch 570/1200, Loss: 0.0737\n",
            "Epoch 2/10, Batch 580/1200, Loss: 0.0697\n",
            "Epoch 2/10, Batch 590/1200, Loss: 0.0758\n",
            "Epoch 2/10, Batch 600/1200, Loss: 0.0715\n",
            "Epoch 2/10, Batch 610/1200, Loss: 0.0740\n",
            "Epoch 2/10, Batch 620/1200, Loss: 0.0635\n",
            "Epoch 2/10, Batch 630/1200, Loss: 0.0736\n",
            "Epoch 2/10, Batch 640/1200, Loss: 0.0686\n",
            "Epoch 2/10, Batch 650/1200, Loss: 0.0733\n",
            "Epoch 2/10, Batch 660/1200, Loss: 0.0735\n",
            "Epoch 2/10, Batch 670/1200, Loss: 0.0766\n",
            "Epoch 2/10, Batch 680/1200, Loss: 0.0733\n",
            "Epoch 2/10, Batch 690/1200, Loss: 0.0711\n",
            "Epoch 2/10, Batch 700/1200, Loss: 0.0711\n",
            "Epoch 2/10, Batch 710/1200, Loss: 0.0758\n",
            "Epoch 2/10, Batch 720/1200, Loss: 0.0754\n",
            "Epoch 2/10, Batch 730/1200, Loss: 0.0712\n",
            "Epoch 2/10, Batch 740/1200, Loss: 0.0770\n",
            "Epoch 2/10, Batch 750/1200, Loss: 0.0711\n",
            "Epoch 2/10, Batch 760/1200, Loss: 0.0741\n",
            "Epoch 2/10, Batch 770/1200, Loss: 0.0727\n",
            "Epoch 2/10, Batch 780/1200, Loss: 0.0751\n",
            "Epoch 2/10, Batch 790/1200, Loss: 0.0756\n",
            "Epoch 2/10, Batch 800/1200, Loss: 0.0761\n",
            "Epoch 2/10, Batch 810/1200, Loss: 0.0685\n",
            "Epoch 2/10, Batch 820/1200, Loss: 0.0717\n",
            "Epoch 2/10, Batch 830/1200, Loss: 0.0744\n",
            "Epoch 2/10, Batch 840/1200, Loss: 0.0714\n",
            "Epoch 2/10, Batch 850/1200, Loss: 0.0753\n",
            "Epoch 2/10, Batch 860/1200, Loss: 0.0721\n",
            "Epoch 2/10, Batch 870/1200, Loss: 0.0678\n",
            "Epoch 2/10, Batch 880/1200, Loss: 0.0720\n",
            "Epoch 2/10, Batch 890/1200, Loss: 0.0684\n",
            "Epoch 2/10, Batch 900/1200, Loss: 0.0712\n",
            "Epoch 2/10, Batch 910/1200, Loss: 0.0771\n",
            "Epoch 2/10, Batch 920/1200, Loss: 0.0708\n",
            "Epoch 2/10, Batch 930/1200, Loss: 0.0735\n",
            "Epoch 2/10, Batch 940/1200, Loss: 0.0704\n",
            "Epoch 2/10, Batch 950/1200, Loss: 0.0685\n",
            "Epoch 2/10, Batch 960/1200, Loss: 0.0649\n",
            "Epoch 2/10, Batch 970/1200, Loss: 0.0717\n",
            "Epoch 2/10, Batch 980/1200, Loss: 0.0757\n",
            "Epoch 2/10, Batch 990/1200, Loss: 0.0704\n",
            "Epoch 2/10, Batch 1000/1200, Loss: 0.0641\n",
            "Epoch 2/10, Batch 1010/1200, Loss: 0.0720\n",
            "Epoch 2/10, Batch 1020/1200, Loss: 0.0671\n",
            "Epoch 2/10, Batch 1030/1200, Loss: 0.0729\n",
            "Epoch 2/10, Batch 1040/1200, Loss: 0.0801\n",
            "Epoch 2/10, Batch 1050/1200, Loss: 0.0703\n",
            "Epoch 2/10, Batch 1060/1200, Loss: 0.0717\n",
            "Epoch 2/10, Batch 1070/1200, Loss: 0.0743\n",
            "Epoch 2/10, Batch 1080/1200, Loss: 0.0779\n",
            "Epoch 2/10, Batch 1090/1200, Loss: 0.0750\n",
            "Epoch 2/10, Batch 1100/1200, Loss: 0.0677\n",
            "Epoch 2/10, Batch 1110/1200, Loss: 0.0746\n",
            "Epoch 2/10, Batch 1120/1200, Loss: 0.0702\n",
            "Epoch 2/10, Batch 1130/1200, Loss: 0.0702\n",
            "Epoch 2/10, Batch 1140/1200, Loss: 0.0757\n",
            "Epoch 2/10, Batch 1150/1200, Loss: 0.0741\n",
            "Epoch 2/10, Batch 1160/1200, Loss: 0.0658\n",
            "Epoch 2/10, Batch 1170/1200, Loss: 0.0746\n",
            "Epoch 2/10, Batch 1180/1200, Loss: 0.0725\n",
            "Epoch 2/10, Batch 1190/1200, Loss: 0.0747\n",
            "Epoch 2 completed, Average Loss: 0.0715\n",
            "Test Loss: 0.0686\n",
            "\n",
            "Epoch 3/10, Batch 0/1200, Loss: 0.0672\n",
            "Epoch 3/10, Batch 10/1200, Loss: 0.0658\n",
            "Epoch 3/10, Batch 20/1200, Loss: 0.0720\n",
            "Epoch 3/10, Batch 30/1200, Loss: 0.0690\n",
            "Epoch 3/10, Batch 40/1200, Loss: 0.0778\n",
            "Epoch 3/10, Batch 50/1200, Loss: 0.0694\n",
            "Epoch 3/10, Batch 60/1200, Loss: 0.0678\n",
            "Epoch 3/10, Batch 70/1200, Loss: 0.0715\n",
            "Epoch 3/10, Batch 80/1200, Loss: 0.0747\n",
            "Epoch 3/10, Batch 90/1200, Loss: 0.0766\n",
            "Epoch 3/10, Batch 100/1200, Loss: 0.0716\n",
            "Epoch 3/10, Batch 110/1200, Loss: 0.0709\n",
            "Epoch 3/10, Batch 120/1200, Loss: 0.0769\n",
            "Epoch 3/10, Batch 130/1200, Loss: 0.0697\n",
            "Epoch 3/10, Batch 140/1200, Loss: 0.0711\n",
            "Epoch 3/10, Batch 150/1200, Loss: 0.0762\n",
            "Epoch 3/10, Batch 160/1200, Loss: 0.0721\n",
            "Epoch 3/10, Batch 170/1200, Loss: 0.0717\n",
            "Epoch 3/10, Batch 180/1200, Loss: 0.0678\n",
            "Epoch 3/10, Batch 190/1200, Loss: 0.0703\n",
            "Epoch 3/10, Batch 200/1200, Loss: 0.0735\n",
            "Epoch 3/10, Batch 210/1200, Loss: 0.0666\n",
            "Epoch 3/10, Batch 220/1200, Loss: 0.0687\n",
            "Epoch 3/10, Batch 230/1200, Loss: 0.0744\n",
            "Epoch 3/10, Batch 240/1200, Loss: 0.0676\n",
            "Epoch 3/10, Batch 250/1200, Loss: 0.0725\n",
            "Epoch 3/10, Batch 260/1200, Loss: 0.0744\n",
            "Epoch 3/10, Batch 270/1200, Loss: 0.0779\n",
            "Epoch 3/10, Batch 280/1200, Loss: 0.0710\n",
            "Epoch 3/10, Batch 290/1200, Loss: 0.0708\n",
            "Epoch 3/10, Batch 300/1200, Loss: 0.0743\n",
            "Epoch 3/10, Batch 310/1200, Loss: 0.0707\n",
            "Epoch 3/10, Batch 320/1200, Loss: 0.0690\n",
            "Epoch 3/10, Batch 330/1200, Loss: 0.0760\n",
            "Epoch 3/10, Batch 340/1200, Loss: 0.0738\n",
            "Epoch 3/10, Batch 350/1200, Loss: 0.0730\n",
            "Epoch 3/10, Batch 360/1200, Loss: 0.0699\n",
            "Epoch 3/10, Batch 370/1200, Loss: 0.0767\n",
            "Epoch 3/10, Batch 380/1200, Loss: 0.0701\n",
            "Epoch 3/10, Batch 390/1200, Loss: 0.0731\n",
            "Epoch 3/10, Batch 400/1200, Loss: 0.0713\n",
            "Epoch 3/10, Batch 410/1200, Loss: 0.0713\n",
            "Epoch 3/10, Batch 420/1200, Loss: 0.0725\n",
            "Epoch 3/10, Batch 430/1200, Loss: 0.0749\n",
            "Epoch 3/10, Batch 440/1200, Loss: 0.0777\n",
            "Epoch 3/10, Batch 450/1200, Loss: 0.0727\n",
            "Epoch 3/10, Batch 460/1200, Loss: 0.0708\n",
            "Epoch 3/10, Batch 470/1200, Loss: 0.0751\n",
            "Epoch 3/10, Batch 480/1200, Loss: 0.0740\n",
            "Epoch 3/10, Batch 490/1200, Loss: 0.0730\n",
            "Epoch 3/10, Batch 500/1200, Loss: 0.0715\n",
            "Epoch 3/10, Batch 510/1200, Loss: 0.0704\n",
            "Epoch 3/10, Batch 520/1200, Loss: 0.0711\n",
            "Epoch 3/10, Batch 530/1200, Loss: 0.0694\n",
            "Epoch 3/10, Batch 540/1200, Loss: 0.0722\n",
            "Epoch 3/10, Batch 550/1200, Loss: 0.0798\n",
            "Epoch 3/10, Batch 560/1200, Loss: 0.0694\n",
            "Epoch 3/10, Batch 570/1200, Loss: 0.0828\n",
            "Epoch 3/10, Batch 580/1200, Loss: 0.0700\n",
            "Epoch 3/10, Batch 590/1200, Loss: 0.0729\n",
            "Epoch 3/10, Batch 600/1200, Loss: 0.0764\n",
            "Epoch 3/10, Batch 610/1200, Loss: 0.0746\n",
            "Epoch 3/10, Batch 620/1200, Loss: 0.0759\n",
            "Epoch 3/10, Batch 630/1200, Loss: 0.0746\n",
            "Epoch 3/10, Batch 640/1200, Loss: 0.0759\n",
            "Epoch 3/10, Batch 650/1200, Loss: 0.0746\n",
            "Epoch 3/10, Batch 660/1200, Loss: 0.0725\n",
            "Epoch 3/10, Batch 670/1200, Loss: 0.0766\n",
            "Epoch 3/10, Batch 680/1200, Loss: 0.0750\n",
            "Epoch 3/10, Batch 690/1200, Loss: 0.0729\n",
            "Epoch 3/10, Batch 700/1200, Loss: 0.0781\n",
            "Epoch 3/10, Batch 710/1200, Loss: 0.0746\n",
            "Epoch 3/10, Batch 720/1200, Loss: 0.0739\n",
            "Epoch 3/10, Batch 730/1200, Loss: 0.0717\n",
            "Epoch 3/10, Batch 740/1200, Loss: 0.0720\n",
            "Epoch 3/10, Batch 750/1200, Loss: 0.0705\n",
            "Epoch 3/10, Batch 760/1200, Loss: 0.0712\n",
            "Epoch 3/10, Batch 770/1200, Loss: 0.0738\n",
            "Epoch 3/10, Batch 780/1200, Loss: 0.0688\n",
            "Epoch 3/10, Batch 790/1200, Loss: 0.0632\n",
            "Epoch 3/10, Batch 800/1200, Loss: 0.0749\n",
            "Epoch 3/10, Batch 810/1200, Loss: 0.0704\n",
            "Epoch 3/10, Batch 820/1200, Loss: 0.0814\n",
            "Epoch 3/10, Batch 830/1200, Loss: 0.0738\n",
            "Epoch 3/10, Batch 840/1200, Loss: 0.0742\n",
            "Epoch 3/10, Batch 850/1200, Loss: 0.0729\n",
            "Epoch 3/10, Batch 860/1200, Loss: 0.0721\n",
            "Epoch 3/10, Batch 870/1200, Loss: 0.0710\n",
            "Epoch 3/10, Batch 880/1200, Loss: 0.0698\n",
            "Epoch 3/10, Batch 890/1200, Loss: 0.0736\n",
            "Epoch 3/10, Batch 900/1200, Loss: 0.0747\n",
            "Epoch 3/10, Batch 910/1200, Loss: 0.0757\n",
            "Epoch 3/10, Batch 920/1200, Loss: 0.0701\n",
            "Epoch 3/10, Batch 930/1200, Loss: 0.0735\n",
            "Epoch 3/10, Batch 940/1200, Loss: 0.0757\n",
            "Epoch 3/10, Batch 950/1200, Loss: 0.0696\n",
            "Epoch 3/10, Batch 960/1200, Loss: 0.0736\n",
            "Epoch 3/10, Batch 970/1200, Loss: 0.0679\n",
            "Epoch 3/10, Batch 980/1200, Loss: 0.0743\n",
            "Epoch 3/10, Batch 990/1200, Loss: 0.0735\n",
            "Epoch 3/10, Batch 1000/1200, Loss: 0.0674\n",
            "Epoch 3/10, Batch 1010/1200, Loss: 0.0722\n",
            "Epoch 3/10, Batch 1020/1200, Loss: 0.0739\n",
            "Epoch 3/10, Batch 1030/1200, Loss: 0.0758\n",
            "Epoch 3/10, Batch 1040/1200, Loss: 0.0697\n",
            "Epoch 3/10, Batch 1050/1200, Loss: 0.0704\n",
            "Epoch 3/10, Batch 1060/1200, Loss: 0.0752\n",
            "Epoch 3/10, Batch 1070/1200, Loss: 0.0749\n",
            "Epoch 3/10, Batch 1080/1200, Loss: 0.0690\n",
            "Epoch 3/10, Batch 1090/1200, Loss: 0.0729\n",
            "Epoch 3/10, Batch 1100/1200, Loss: 0.0738\n",
            "Epoch 3/10, Batch 1110/1200, Loss: 0.0693\n",
            "Epoch 3/10, Batch 1120/1200, Loss: 0.0759\n",
            "Epoch 3/10, Batch 1130/1200, Loss: 0.0697\n",
            "Epoch 3/10, Batch 1140/1200, Loss: 0.0728\n",
            "Epoch 3/10, Batch 1150/1200, Loss: 0.0713\n",
            "Epoch 3/10, Batch 1160/1200, Loss: 0.0686\n",
            "Epoch 3/10, Batch 1170/1200, Loss: 0.0730\n",
            "Epoch 3/10, Batch 1180/1200, Loss: 0.0739\n",
            "Epoch 3/10, Batch 1190/1200, Loss: 0.0705\n",
            "Epoch 3 completed, Average Loss: 0.0724\n",
            "Test Loss: 0.0714\n",
            "\n",
            "Epoch 4/10, Batch 0/1200, Loss: 0.0769\n",
            "Epoch 4/10, Batch 10/1200, Loss: 0.0729\n",
            "Epoch 4/10, Batch 20/1200, Loss: 0.0691\n",
            "Epoch 4/10, Batch 30/1200, Loss: 0.0770\n",
            "Epoch 4/10, Batch 40/1200, Loss: 0.0780\n",
            "Epoch 4/10, Batch 50/1200, Loss: 0.0715\n",
            "Epoch 4/10, Batch 60/1200, Loss: 0.0734\n",
            "Epoch 4/10, Batch 70/1200, Loss: 0.0693\n",
            "Epoch 4/10, Batch 80/1200, Loss: 0.0748\n",
            "Epoch 4/10, Batch 90/1200, Loss: 0.0765\n",
            "Epoch 4/10, Batch 100/1200, Loss: 0.0720\n",
            "Epoch 4/10, Batch 110/1200, Loss: 0.0680\n",
            "Epoch 4/10, Batch 120/1200, Loss: 0.0770\n",
            "Epoch 4/10, Batch 130/1200, Loss: 0.0731\n",
            "Epoch 4/10, Batch 140/1200, Loss: 0.0712\n",
            "Epoch 4/10, Batch 150/1200, Loss: 0.0765\n",
            "Epoch 4/10, Batch 160/1200, Loss: 0.0743\n",
            "Epoch 4/10, Batch 170/1200, Loss: 0.0724\n",
            "Epoch 4/10, Batch 180/1200, Loss: 0.0762\n",
            "Epoch 4/10, Batch 190/1200, Loss: 0.0687\n",
            "Epoch 4/10, Batch 200/1200, Loss: 0.0692\n",
            "Epoch 4/10, Batch 210/1200, Loss: 0.0746\n",
            "Epoch 4/10, Batch 220/1200, Loss: 0.0792\n",
            "Epoch 4/10, Batch 230/1200, Loss: 0.0753\n",
            "Epoch 4/10, Batch 240/1200, Loss: 0.0738\n",
            "Epoch 4/10, Batch 250/1200, Loss: 0.0732\n",
            "Epoch 4/10, Batch 260/1200, Loss: 0.0711\n",
            "Epoch 4/10, Batch 270/1200, Loss: 0.0670\n",
            "Epoch 4/10, Batch 280/1200, Loss: 0.0742\n",
            "Epoch 4/10, Batch 290/1200, Loss: 0.0723\n",
            "Epoch 4/10, Batch 300/1200, Loss: 0.0749\n",
            "Epoch 4/10, Batch 310/1200, Loss: 0.0699\n",
            "Epoch 4/10, Batch 320/1200, Loss: 0.0727\n",
            "Epoch 4/10, Batch 330/1200, Loss: 0.0674\n",
            "Epoch 4/10, Batch 340/1200, Loss: 0.0726\n",
            "Epoch 4/10, Batch 350/1200, Loss: 0.0773\n",
            "Epoch 4/10, Batch 360/1200, Loss: 0.0746\n",
            "Epoch 4/10, Batch 370/1200, Loss: 0.0732\n",
            "Epoch 4/10, Batch 380/1200, Loss: 0.0744\n",
            "Epoch 4/10, Batch 390/1200, Loss: 0.0723\n",
            "Epoch 4/10, Batch 400/1200, Loss: 0.0723\n",
            "Epoch 4/10, Batch 410/1200, Loss: 0.0772\n",
            "Epoch 4/10, Batch 420/1200, Loss: 0.0729\n",
            "Epoch 4/10, Batch 430/1200, Loss: 0.0683\n",
            "Epoch 4/10, Batch 440/1200, Loss: 0.0734\n",
            "Epoch 4/10, Batch 450/1200, Loss: 0.0751\n",
            "Epoch 4/10, Batch 460/1200, Loss: 0.0735\n",
            "Epoch 4/10, Batch 470/1200, Loss: 0.0706\n",
            "Epoch 4/10, Batch 480/1200, Loss: 0.0728\n",
            "Epoch 4/10, Batch 490/1200, Loss: 0.0865\n",
            "Epoch 4/10, Batch 500/1200, Loss: 0.0768\n",
            "Epoch 4/10, Batch 510/1200, Loss: 0.0782\n",
            "Epoch 4/10, Batch 520/1200, Loss: 0.0739\n",
            "Epoch 4/10, Batch 530/1200, Loss: 0.0785\n",
            "Epoch 4/10, Batch 540/1200, Loss: 0.0741\n",
            "Epoch 4/10, Batch 550/1200, Loss: 0.0731\n",
            "Epoch 4/10, Batch 560/1200, Loss: 0.0772\n",
            "Epoch 4/10, Batch 570/1200, Loss: 0.0732\n",
            "Epoch 4/10, Batch 580/1200, Loss: 0.0772\n",
            "Epoch 4/10, Batch 590/1200, Loss: 0.0739\n",
            "Epoch 4/10, Batch 600/1200, Loss: 0.0730\n",
            "Epoch 4/10, Batch 610/1200, Loss: 0.0702\n",
            "Epoch 4/10, Batch 620/1200, Loss: 0.0700\n",
            "Epoch 4/10, Batch 630/1200, Loss: 0.0693\n",
            "Epoch 4/10, Batch 640/1200, Loss: 0.0756\n",
            "Epoch 4/10, Batch 650/1200, Loss: 0.0767\n",
            "Epoch 4/10, Batch 660/1200, Loss: 0.0696\n",
            "Epoch 4/10, Batch 670/1200, Loss: 0.0729\n",
            "Epoch 4/10, Batch 680/1200, Loss: 0.0693\n",
            "Epoch 4/10, Batch 690/1200, Loss: 0.0738\n",
            "Epoch 4/10, Batch 700/1200, Loss: 0.0717\n",
            "Epoch 4/10, Batch 710/1200, Loss: 0.0700\n",
            "Epoch 4/10, Batch 720/1200, Loss: 0.0699\n",
            "Epoch 4/10, Batch 730/1200, Loss: 0.0795\n",
            "Epoch 4/10, Batch 740/1200, Loss: 0.0710\n",
            "Epoch 4/10, Batch 750/1200, Loss: 0.0723\n",
            "Epoch 4/10, Batch 760/1200, Loss: 0.0788\n",
            "Epoch 4/10, Batch 770/1200, Loss: 0.0737\n",
            "Epoch 4/10, Batch 780/1200, Loss: 0.0710\n",
            "Epoch 4/10, Batch 790/1200, Loss: 0.0726\n",
            "Epoch 4/10, Batch 800/1200, Loss: 0.0789\n",
            "Epoch 4/10, Batch 810/1200, Loss: 0.0718\n",
            "Epoch 4/10, Batch 820/1200, Loss: 0.0763\n",
            "Epoch 4/10, Batch 830/1200, Loss: 0.0755\n",
            "Epoch 4/10, Batch 840/1200, Loss: 0.0677\n",
            "Epoch 4/10, Batch 850/1200, Loss: 0.0715\n",
            "Epoch 4/10, Batch 860/1200, Loss: 0.0791\n",
            "Epoch 4/10, Batch 870/1200, Loss: 0.0751\n",
            "Epoch 4/10, Batch 880/1200, Loss: 0.0715\n",
            "Epoch 4/10, Batch 890/1200, Loss: 0.0720\n",
            "Epoch 4/10, Batch 900/1200, Loss: 0.0794\n",
            "Epoch 4/10, Batch 910/1200, Loss: 0.0722\n",
            "Epoch 4/10, Batch 920/1200, Loss: 0.0765\n",
            "Epoch 4/10, Batch 930/1200, Loss: 0.0752\n",
            "Epoch 4/10, Batch 940/1200, Loss: 0.0699\n",
            "Epoch 4/10, Batch 950/1200, Loss: 0.0693\n",
            "Epoch 4/10, Batch 960/1200, Loss: 0.0720\n",
            "Epoch 4/10, Batch 970/1200, Loss: 0.0737\n",
            "Epoch 4/10, Batch 980/1200, Loss: 0.0731\n",
            "Epoch 4/10, Batch 990/1200, Loss: 0.0737\n",
            "Epoch 4/10, Batch 1000/1200, Loss: 0.0682\n",
            "Epoch 4/10, Batch 1010/1200, Loss: 0.0771\n",
            "Epoch 4/10, Batch 1020/1200, Loss: 0.0676\n",
            "Epoch 4/10, Batch 1030/1200, Loss: 0.0739\n",
            "Epoch 4/10, Batch 1040/1200, Loss: 0.0720\n",
            "Epoch 4/10, Batch 1050/1200, Loss: 0.0721\n",
            "Epoch 4/10, Batch 1060/1200, Loss: 0.0715\n",
            "Epoch 4/10, Batch 1070/1200, Loss: 0.0721\n",
            "Epoch 4/10, Batch 1080/1200, Loss: 0.0696\n",
            "Epoch 4/10, Batch 1090/1200, Loss: 0.0755\n",
            "Epoch 4/10, Batch 1100/1200, Loss: 0.0701\n",
            "Epoch 4/10, Batch 1110/1200, Loss: 0.0675\n",
            "Epoch 4/10, Batch 1120/1200, Loss: 0.0757\n",
            "Epoch 4/10, Batch 1130/1200, Loss: 0.0813\n",
            "Epoch 4/10, Batch 1140/1200, Loss: 0.0749\n",
            "Epoch 4/10, Batch 1150/1200, Loss: 0.0717\n",
            "Epoch 4/10, Batch 1160/1200, Loss: 0.0802\n",
            "Epoch 4/10, Batch 1170/1200, Loss: 0.0753\n",
            "Epoch 4/10, Batch 1180/1200, Loss: 0.0714\n",
            "Epoch 4/10, Batch 1190/1200, Loss: 0.0745\n",
            "Epoch 4 completed, Average Loss: 0.0730\n",
            "Test Loss: 0.0718\n",
            "\n",
            "Epoch 5/10, Batch 0/1200, Loss: 0.0741\n",
            "Epoch 5/10, Batch 10/1200, Loss: 0.0708\n",
            "Epoch 5/10, Batch 20/1200, Loss: 0.0752\n",
            "Epoch 5/10, Batch 30/1200, Loss: 0.0783\n",
            "Epoch 5/10, Batch 40/1200, Loss: 0.0742\n",
            "Epoch 5/10, Batch 50/1200, Loss: 0.0722\n",
            "Epoch 5/10, Batch 60/1200, Loss: 0.0672\n",
            "Epoch 5/10, Batch 70/1200, Loss: 0.0717\n",
            "Epoch 5/10, Batch 80/1200, Loss: 0.0726\n",
            "Epoch 5/10, Batch 90/1200, Loss: 0.0731\n",
            "Epoch 5/10, Batch 100/1200, Loss: 0.0726\n",
            "Epoch 5/10, Batch 110/1200, Loss: 0.0711\n",
            "Epoch 5/10, Batch 120/1200, Loss: 0.0754\n",
            "Epoch 5/10, Batch 130/1200, Loss: 0.0761\n",
            "Epoch 5/10, Batch 140/1200, Loss: 0.0712\n",
            "Epoch 5/10, Batch 150/1200, Loss: 0.0675\n",
            "Epoch 5/10, Batch 160/1200, Loss: 0.0713\n",
            "Epoch 5/10, Batch 170/1200, Loss: 0.0700\n",
            "Epoch 5/10, Batch 180/1200, Loss: 0.0748\n",
            "Epoch 5/10, Batch 190/1200, Loss: 0.0804\n",
            "Epoch 5/10, Batch 200/1200, Loss: 0.0739\n",
            "Epoch 5/10, Batch 210/1200, Loss: 0.0759\n",
            "Epoch 5/10, Batch 220/1200, Loss: 0.0746\n",
            "Epoch 5/10, Batch 230/1200, Loss: 0.0745\n",
            "Epoch 5/10, Batch 240/1200, Loss: 0.0771\n",
            "Epoch 5/10, Batch 250/1200, Loss: 0.0683\n",
            "Epoch 5/10, Batch 260/1200, Loss: 0.0683\n",
            "Epoch 5/10, Batch 270/1200, Loss: 0.0718\n",
            "Epoch 5/10, Batch 280/1200, Loss: 0.0752\n",
            "Epoch 5/10, Batch 290/1200, Loss: 0.0722\n",
            "Epoch 5/10, Batch 300/1200, Loss: 0.0757\n",
            "Epoch 5/10, Batch 310/1200, Loss: 0.0789\n",
            "Epoch 5/10, Batch 320/1200, Loss: 0.0734\n",
            "Epoch 5/10, Batch 330/1200, Loss: 0.0706\n",
            "Epoch 5/10, Batch 340/1200, Loss: 0.0738\n",
            "Epoch 5/10, Batch 350/1200, Loss: 0.0739\n",
            "Epoch 5/10, Batch 360/1200, Loss: 0.0721\n",
            "Epoch 5/10, Batch 370/1200, Loss: 0.0786\n",
            "Epoch 5/10, Batch 380/1200, Loss: 0.0802\n",
            "Epoch 5/10, Batch 390/1200, Loss: 0.0756\n",
            "Epoch 5/10, Batch 400/1200, Loss: 0.0708\n",
            "Epoch 5/10, Batch 410/1200, Loss: 0.0693\n",
            "Epoch 5/10, Batch 420/1200, Loss: 0.0718\n",
            "Epoch 5/10, Batch 430/1200, Loss: 0.0755\n",
            "Epoch 5/10, Batch 440/1200, Loss: 0.0755\n",
            "Epoch 5/10, Batch 450/1200, Loss: 0.0825\n",
            "Epoch 5/10, Batch 460/1200, Loss: 0.0705\n",
            "Epoch 5/10, Batch 470/1200, Loss: 0.0688\n",
            "Epoch 5/10, Batch 480/1200, Loss: 0.0750\n",
            "Epoch 5/10, Batch 490/1200, Loss: 0.0743\n",
            "Epoch 5/10, Batch 500/1200, Loss: 0.0693\n",
            "Epoch 5/10, Batch 510/1200, Loss: 0.0740\n",
            "Epoch 5/10, Batch 520/1200, Loss: 0.0674\n",
            "Epoch 5/10, Batch 530/1200, Loss: 0.0742\n",
            "Epoch 5/10, Batch 540/1200, Loss: 0.0761\n",
            "Epoch 5/10, Batch 550/1200, Loss: 0.0718\n",
            "Epoch 5/10, Batch 560/1200, Loss: 0.0707\n",
            "Epoch 5/10, Batch 570/1200, Loss: 0.0727\n",
            "Epoch 5/10, Batch 580/1200, Loss: 0.0759\n",
            "Epoch 5/10, Batch 590/1200, Loss: 0.0743\n",
            "Epoch 5/10, Batch 600/1200, Loss: 0.0705\n",
            "Epoch 5/10, Batch 610/1200, Loss: 0.0722\n",
            "Epoch 5/10, Batch 620/1200, Loss: 0.0720\n",
            "Epoch 5/10, Batch 630/1200, Loss: 0.0726\n",
            "Epoch 5/10, Batch 640/1200, Loss: 0.0696\n",
            "Epoch 5/10, Batch 650/1200, Loss: 0.0726\n",
            "Epoch 5/10, Batch 660/1200, Loss: 0.0732\n",
            "Epoch 5/10, Batch 670/1200, Loss: 0.0725\n",
            "Epoch 5/10, Batch 680/1200, Loss: 0.0733\n",
            "Epoch 5/10, Batch 690/1200, Loss: 0.0740\n",
            "Epoch 5/10, Batch 700/1200, Loss: 0.0713\n",
            "Epoch 5/10, Batch 710/1200, Loss: 0.0684\n",
            "Epoch 5/10, Batch 720/1200, Loss: 0.0697\n",
            "Epoch 5/10, Batch 730/1200, Loss: 0.0743\n",
            "Epoch 5/10, Batch 740/1200, Loss: 0.0730\n",
            "Epoch 5/10, Batch 750/1200, Loss: 0.0776\n",
            "Epoch 5/10, Batch 760/1200, Loss: 0.0749\n",
            "Epoch 5/10, Batch 770/1200, Loss: 0.0747\n",
            "Epoch 5/10, Batch 780/1200, Loss: 0.0816\n",
            "Epoch 5/10, Batch 790/1200, Loss: 0.0715\n",
            "Epoch 5/10, Batch 800/1200, Loss: 0.0697\n",
            "Epoch 5/10, Batch 810/1200, Loss: 0.0744\n",
            "Epoch 5/10, Batch 820/1200, Loss: 0.0723\n",
            "Epoch 5/10, Batch 830/1200, Loss: 0.0775\n",
            "Epoch 5/10, Batch 840/1200, Loss: 0.0741\n",
            "Epoch 5/10, Batch 850/1200, Loss: 0.0780\n",
            "Epoch 5/10, Batch 860/1200, Loss: 0.0800\n",
            "Epoch 5/10, Batch 870/1200, Loss: 0.0750\n",
            "Epoch 5/10, Batch 880/1200, Loss: 0.0756\n",
            "Epoch 5/10, Batch 890/1200, Loss: 0.0683\n",
            "Epoch 5/10, Batch 900/1200, Loss: 0.0774\n",
            "Epoch 5/10, Batch 910/1200, Loss: 0.0751\n",
            "Epoch 5/10, Batch 920/1200, Loss: 0.0737\n",
            "Epoch 5/10, Batch 930/1200, Loss: 0.0712\n",
            "Epoch 5/10, Batch 940/1200, Loss: 0.0741\n",
            "Epoch 5/10, Batch 950/1200, Loss: 0.0743\n",
            "Epoch 5/10, Batch 960/1200, Loss: 0.0756\n",
            "Epoch 5/10, Batch 970/1200, Loss: 0.0802\n",
            "Epoch 5/10, Batch 980/1200, Loss: 0.0739\n",
            "Epoch 5/10, Batch 990/1200, Loss: 0.0737\n",
            "Epoch 5/10, Batch 1000/1200, Loss: 0.0772\n",
            "Epoch 5/10, Batch 1010/1200, Loss: 0.0747\n",
            "Epoch 5/10, Batch 1020/1200, Loss: 0.0719\n",
            "Epoch 5/10, Batch 1030/1200, Loss: 0.0736\n",
            "Epoch 5/10, Batch 1040/1200, Loss: 0.0683\n",
            "Epoch 5/10, Batch 1050/1200, Loss: 0.0702\n",
            "Epoch 5/10, Batch 1060/1200, Loss: 0.0759\n",
            "Epoch 5/10, Batch 1070/1200, Loss: 0.0723\n",
            "Epoch 5/10, Batch 1080/1200, Loss: 0.0747\n",
            "Epoch 5/10, Batch 1090/1200, Loss: 0.0799\n",
            "Epoch 5/10, Batch 1100/1200, Loss: 0.0716\n",
            "Epoch 5/10, Batch 1110/1200, Loss: 0.0753\n",
            "Epoch 5/10, Batch 1120/1200, Loss: 0.0748\n",
            "Epoch 5/10, Batch 1130/1200, Loss: 0.0747\n",
            "Epoch 5/10, Batch 1140/1200, Loss: 0.0777\n",
            "Epoch 5/10, Batch 1150/1200, Loss: 0.0726\n",
            "Epoch 5/10, Batch 1160/1200, Loss: 0.0746\n",
            "Epoch 5/10, Batch 1170/1200, Loss: 0.0748\n",
            "Epoch 5/10, Batch 1180/1200, Loss: 0.0737\n",
            "Epoch 5/10, Batch 1190/1200, Loss: 0.0775\n",
            "Epoch 5 completed, Average Loss: 0.0735\n",
            "Test Loss: 0.0726\n",
            "\n",
            "Epoch 6/10, Batch 0/1200, Loss: 0.0740\n",
            "Epoch 6/10, Batch 10/1200, Loss: 0.0754\n",
            "Epoch 6/10, Batch 20/1200, Loss: 0.0794\n",
            "Epoch 6/10, Batch 30/1200, Loss: 0.0723\n",
            "Epoch 6/10, Batch 40/1200, Loss: 0.0711\n",
            "Epoch 6/10, Batch 50/1200, Loss: 0.0808\n",
            "Epoch 6/10, Batch 60/1200, Loss: 0.0697\n",
            "Epoch 6/10, Batch 70/1200, Loss: 0.0687\n",
            "Epoch 6/10, Batch 80/1200, Loss: 0.0766\n",
            "Epoch 6/10, Batch 90/1200, Loss: 0.0720\n",
            "Epoch 6/10, Batch 100/1200, Loss: 0.0715\n",
            "Epoch 6/10, Batch 110/1200, Loss: 0.0754\n",
            "Epoch 6/10, Batch 120/1200, Loss: 0.0750\n",
            "Epoch 6/10, Batch 130/1200, Loss: 0.0756\n",
            "Epoch 6/10, Batch 140/1200, Loss: 0.0700\n",
            "Epoch 6/10, Batch 150/1200, Loss: 0.0758\n",
            "Epoch 6/10, Batch 160/1200, Loss: 0.0747\n",
            "Epoch 6/10, Batch 170/1200, Loss: 0.0752\n",
            "Epoch 6/10, Batch 180/1200, Loss: 0.0803\n",
            "Epoch 6/10, Batch 190/1200, Loss: 0.0732\n",
            "Epoch 6/10, Batch 200/1200, Loss: 0.0759\n",
            "Epoch 6/10, Batch 210/1200, Loss: 0.0677\n",
            "Epoch 6/10, Batch 220/1200, Loss: 0.0725\n",
            "Epoch 6/10, Batch 230/1200, Loss: 0.0790\n",
            "Epoch 6/10, Batch 240/1200, Loss: 0.0733\n",
            "Epoch 6/10, Batch 250/1200, Loss: 0.0696\n",
            "Epoch 6/10, Batch 260/1200, Loss: 0.0784\n",
            "Epoch 6/10, Batch 270/1200, Loss: 0.0726\n",
            "Epoch 6/10, Batch 280/1200, Loss: 0.0806\n",
            "Epoch 6/10, Batch 290/1200, Loss: 0.0753\n",
            "Epoch 6/10, Batch 300/1200, Loss: 0.0726\n",
            "Epoch 6/10, Batch 310/1200, Loss: 0.0740\n",
            "Epoch 6/10, Batch 320/1200, Loss: 0.0772\n",
            "Epoch 6/10, Batch 330/1200, Loss: 0.0788\n",
            "Epoch 6/10, Batch 340/1200, Loss: 0.0800\n",
            "Epoch 6/10, Batch 350/1200, Loss: 0.0716\n",
            "Epoch 6/10, Batch 360/1200, Loss: 0.0730\n",
            "Epoch 6/10, Batch 370/1200, Loss: 0.0692\n",
            "Epoch 6/10, Batch 380/1200, Loss: 0.0779\n",
            "Epoch 6/10, Batch 390/1200, Loss: 0.0756\n",
            "Epoch 6/10, Batch 400/1200, Loss: 0.0767\n",
            "Epoch 6/10, Batch 410/1200, Loss: 0.0756\n",
            "Epoch 6/10, Batch 420/1200, Loss: 0.0765\n",
            "Epoch 6/10, Batch 430/1200, Loss: 0.0694\n",
            "Epoch 6/10, Batch 440/1200, Loss: 0.0729\n",
            "Epoch 6/10, Batch 450/1200, Loss: 0.0792\n",
            "Epoch 6/10, Batch 460/1200, Loss: 0.0794\n",
            "Epoch 6/10, Batch 470/1200, Loss: 0.0715\n",
            "Epoch 6/10, Batch 480/1200, Loss: 0.0756\n",
            "Epoch 6/10, Batch 490/1200, Loss: 0.0739\n",
            "Epoch 6/10, Batch 500/1200, Loss: 0.0708\n",
            "Epoch 6/10, Batch 510/1200, Loss: 0.0748\n",
            "Epoch 6/10, Batch 520/1200, Loss: 0.0796\n",
            "Epoch 6/10, Batch 530/1200, Loss: 0.0769\n",
            "Epoch 6/10, Batch 540/1200, Loss: 0.0774\n",
            "Epoch 6/10, Batch 550/1200, Loss: 0.0733\n",
            "Epoch 6/10, Batch 560/1200, Loss: 0.0758\n",
            "Epoch 6/10, Batch 570/1200, Loss: 0.0767\n",
            "Epoch 6/10, Batch 580/1200, Loss: 0.0714\n",
            "Epoch 6/10, Batch 590/1200, Loss: 0.0709\n",
            "Epoch 6/10, Batch 600/1200, Loss: 0.0712\n",
            "Epoch 6/10, Batch 610/1200, Loss: 0.0743\n",
            "Epoch 6/10, Batch 620/1200, Loss: 0.0741\n",
            "Epoch 6/10, Batch 630/1200, Loss: 0.0732\n",
            "Epoch 6/10, Batch 640/1200, Loss: 0.0742\n",
            "Epoch 6/10, Batch 650/1200, Loss: 0.0696\n",
            "Epoch 6/10, Batch 660/1200, Loss: 0.0698\n",
            "Epoch 6/10, Batch 670/1200, Loss: 0.0785\n",
            "Epoch 6/10, Batch 680/1200, Loss: 0.0710\n",
            "Epoch 6/10, Batch 690/1200, Loss: 0.0719\n",
            "Epoch 6/10, Batch 700/1200, Loss: 0.0773\n",
            "Epoch 6/10, Batch 710/1200, Loss: 0.0719\n",
            "Epoch 6/10, Batch 720/1200, Loss: 0.0718\n",
            "Epoch 6/10, Batch 730/1200, Loss: 0.0727\n",
            "Epoch 6/10, Batch 740/1200, Loss: 0.0722\n",
            "Epoch 6/10, Batch 750/1200, Loss: 0.0781\n",
            "Epoch 6/10, Batch 760/1200, Loss: 0.0706\n",
            "Epoch 6/10, Batch 770/1200, Loss: 0.0687\n",
            "Epoch 6/10, Batch 780/1200, Loss: 0.0740\n",
            "Epoch 6/10, Batch 790/1200, Loss: 0.0674\n",
            "Epoch 6/10, Batch 800/1200, Loss: 0.0762\n",
            "Epoch 6/10, Batch 810/1200, Loss: 0.0697\n",
            "Epoch 6/10, Batch 820/1200, Loss: 0.0761\n",
            "Epoch 6/10, Batch 830/1200, Loss: 0.0738\n",
            "Epoch 6/10, Batch 840/1200, Loss: 0.0772\n",
            "Epoch 6/10, Batch 850/1200, Loss: 0.0772\n",
            "Epoch 6/10, Batch 860/1200, Loss: 0.0773\n",
            "Epoch 6/10, Batch 870/1200, Loss: 0.0710\n",
            "Epoch 6/10, Batch 880/1200, Loss: 0.0724\n",
            "Epoch 6/10, Batch 890/1200, Loss: 0.0677\n",
            "Epoch 6/10, Batch 900/1200, Loss: 0.0733\n",
            "Epoch 6/10, Batch 910/1200, Loss: 0.0740\n",
            "Epoch 6/10, Batch 920/1200, Loss: 0.0708\n",
            "Epoch 6/10, Batch 930/1200, Loss: 0.0768\n",
            "Epoch 6/10, Batch 940/1200, Loss: 0.0738\n",
            "Epoch 6/10, Batch 950/1200, Loss: 0.0758\n",
            "Epoch 6/10, Batch 960/1200, Loss: 0.0742\n",
            "Epoch 6/10, Batch 970/1200, Loss: 0.0765\n",
            "Epoch 6/10, Batch 980/1200, Loss: 0.0782\n",
            "Epoch 6/10, Batch 990/1200, Loss: 0.0748\n",
            "Epoch 6/10, Batch 1000/1200, Loss: 0.0767\n",
            "Epoch 6/10, Batch 1010/1200, Loss: 0.0702\n",
            "Epoch 6/10, Batch 1020/1200, Loss: 0.0723\n",
            "Epoch 6/10, Batch 1030/1200, Loss: 0.0756\n",
            "Epoch 6/10, Batch 1040/1200, Loss: 0.0735\n",
            "Epoch 6/10, Batch 1050/1200, Loss: 0.0739\n",
            "Epoch 6/10, Batch 1060/1200, Loss: 0.0729\n",
            "Epoch 6/10, Batch 1070/1200, Loss: 0.0743\n",
            "Epoch 6/10, Batch 1080/1200, Loss: 0.0747\n",
            "Epoch 6/10, Batch 1090/1200, Loss: 0.0693\n",
            "Epoch 6/10, Batch 1100/1200, Loss: 0.0791\n",
            "Epoch 6/10, Batch 1110/1200, Loss: 0.0680\n",
            "Epoch 6/10, Batch 1120/1200, Loss: 0.0747\n",
            "Epoch 6/10, Batch 1130/1200, Loss: 0.0725\n",
            "Epoch 6/10, Batch 1140/1200, Loss: 0.0741\n",
            "Epoch 6/10, Batch 1150/1200, Loss: 0.0706\n",
            "Epoch 6/10, Batch 1160/1200, Loss: 0.0753\n",
            "Epoch 6/10, Batch 1170/1200, Loss: 0.0749\n",
            "Epoch 6/10, Batch 1180/1200, Loss: 0.0764\n",
            "Epoch 6/10, Batch 1190/1200, Loss: 0.0783\n",
            "Epoch 6 completed, Average Loss: 0.0740\n",
            "Test Loss: 0.0698\n",
            "\n",
            "Epoch 7/10, Batch 0/1200, Loss: 0.0734\n",
            "Epoch 7/10, Batch 10/1200, Loss: 0.0800\n",
            "Epoch 7/10, Batch 20/1200, Loss: 0.0766\n",
            "Epoch 7/10, Batch 30/1200, Loss: 0.0641\n",
            "Epoch 7/10, Batch 40/1200, Loss: 0.0704\n",
            "Epoch 7/10, Batch 50/1200, Loss: 0.0770\n",
            "Epoch 7/10, Batch 60/1200, Loss: 0.0724\n",
            "Epoch 7/10, Batch 70/1200, Loss: 0.0720\n",
            "Epoch 7/10, Batch 80/1200, Loss: 0.0812\n",
            "Epoch 7/10, Batch 90/1200, Loss: 0.0806\n",
            "Epoch 7/10, Batch 100/1200, Loss: 0.0739\n",
            "Epoch 7/10, Batch 110/1200, Loss: 0.0694\n",
            "Epoch 7/10, Batch 120/1200, Loss: 0.0775\n",
            "Epoch 7/10, Batch 130/1200, Loss: 0.0754\n",
            "Epoch 7/10, Batch 140/1200, Loss: 0.0725\n",
            "Epoch 7/10, Batch 150/1200, Loss: 0.0800\n",
            "Epoch 7/10, Batch 160/1200, Loss: 0.0773\n",
            "Epoch 7/10, Batch 170/1200, Loss: 0.0732\n",
            "Epoch 7/10, Batch 180/1200, Loss: 0.0748\n",
            "Epoch 7/10, Batch 190/1200, Loss: 0.0777\n",
            "Epoch 7/10, Batch 200/1200, Loss: 0.0749\n",
            "Epoch 7/10, Batch 210/1200, Loss: 0.0714\n",
            "Epoch 7/10, Batch 220/1200, Loss: 0.0726\n",
            "Epoch 7/10, Batch 230/1200, Loss: 0.0745\n",
            "Epoch 7/10, Batch 240/1200, Loss: 0.0736\n",
            "Epoch 7/10, Batch 250/1200, Loss: 0.0774\n",
            "Epoch 7/10, Batch 260/1200, Loss: 0.0813\n",
            "Epoch 7/10, Batch 270/1200, Loss: 0.0730\n",
            "Epoch 7/10, Batch 280/1200, Loss: 0.0749\n",
            "Epoch 7/10, Batch 290/1200, Loss: 0.0771\n",
            "Epoch 7/10, Batch 300/1200, Loss: 0.0798\n",
            "Epoch 7/10, Batch 310/1200, Loss: 0.0744\n",
            "Epoch 7/10, Batch 320/1200, Loss: 0.0714\n",
            "Epoch 7/10, Batch 330/1200, Loss: 0.0775\n",
            "Epoch 7/10, Batch 340/1200, Loss: 0.0732\n",
            "Epoch 7/10, Batch 350/1200, Loss: 0.0767\n",
            "Epoch 7/10, Batch 360/1200, Loss: 0.0710\n",
            "Epoch 7/10, Batch 370/1200, Loss: 0.0741\n",
            "Epoch 7/10, Batch 380/1200, Loss: 0.0758\n",
            "Epoch 7/10, Batch 390/1200, Loss: 0.0721\n",
            "Epoch 7/10, Batch 400/1200, Loss: 0.0720\n",
            "Epoch 7/10, Batch 410/1200, Loss: 0.0701\n",
            "Epoch 7/10, Batch 420/1200, Loss: 0.0709\n",
            "Epoch 7/10, Batch 430/1200, Loss: 0.0723\n",
            "Epoch 7/10, Batch 440/1200, Loss: 0.0704\n",
            "Epoch 7/10, Batch 450/1200, Loss: 0.0761\n",
            "Epoch 7/10, Batch 460/1200, Loss: 0.0747\n",
            "Epoch 7/10, Batch 470/1200, Loss: 0.0763\n",
            "Epoch 7/10, Batch 480/1200, Loss: 0.0690\n",
            "Epoch 7/10, Batch 490/1200, Loss: 0.0756\n",
            "Epoch 7/10, Batch 500/1200, Loss: 0.0765\n",
            "Epoch 7/10, Batch 510/1200, Loss: 0.0748\n",
            "Epoch 7/10, Batch 520/1200, Loss: 0.0772\n",
            "Epoch 7/10, Batch 530/1200, Loss: 0.0804\n",
            "Epoch 7/10, Batch 540/1200, Loss: 0.0720\n",
            "Epoch 7/10, Batch 550/1200, Loss: 0.0785\n",
            "Epoch 7/10, Batch 560/1200, Loss: 0.0731\n",
            "Epoch 7/10, Batch 570/1200, Loss: 0.0759\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-cc3419cee821>\u001b[0m in \u001b[0;36m<cell line: 285>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m     \u001b[0mtrain_autoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-cc3419cee821>\u001b[0m in \u001b[0;36mtrain_autoencoder\u001b[0;34m()\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0;31m# Train on batch using LRA updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_lraupdates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-cc3419cee821>\u001b[0m in \u001b[0;36mcompute_lraupdates\u001b[0;34m(self, x_train)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;31m# Apply updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;31m# Return current loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars)\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m         \u001b[0;31m# Return iterations for compat with tf.keras.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, grads, trainable_variables)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;31m# Apply gradient updates.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend_apply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m             \u001b[0;31m# Apply variable constraints after applying gradients.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mvariable\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py\u001b[0m in \u001b[0;36m_backend_apply_gradients\u001b[0;34m(self, grads, trainable_variables)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;31m# Run udpate step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m             self._backend_update_step(\n\u001b[0m\u001b[1;32m    420\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/optimizer.py\u001b[0m in \u001b[0;36m_backend_update_step\u001b[0;34m(self, grads, trainable_variables, learning_rate)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_all_reduce_sum_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         tf.__internal__.distribute.interim.maybe_merge_call(\n\u001b[0m\u001b[1;32m    122\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distributed_tf_update_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution_strategy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/merge_call_interim.py\u001b[0m in \u001b[0;36mmaybe_merge_call\u001b[0;34m(fn, strategy, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m   \"\"\"\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mstrategy_supports_no_merge_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     return distribute_lib.get_replica_context().merge_call(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/optimizer.py\u001b[0m in \u001b[0;36m_distributed_tf_update_step\u001b[0;34m(self, distribution, grads_and_vars, learning_rate)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             distribution.extended.update(\n\u001b[0m\u001b[1;32m    136\u001b[0m                 \u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                 \u001b[0mapply_grad_to_update_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   3003\u001b[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[1;32m   3004\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3005\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3006\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3007\u001b[0m       return self._replica_ctx_update(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   4073\u001b[0m     \u001b[0;31m# The implementations of _update() and _update_non_slot() are identical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4074\u001b[0m     \u001b[0;31m# except _update() passes `var` as the first argument to `fn()`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4075\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_non_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4076\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4077\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_update_non_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolocate_with\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_update_non_slot\u001b[0;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[1;32m   4079\u001b[0m     \u001b[0;31m# once that value is used for something.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4080\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mUpdateContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4081\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4082\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4083\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNSPECIFIED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/optimizer.py\u001b[0m in \u001b[0;36mapply_grad_to_update_var\u001b[0;34m(var, grad, learning_rate)\u001b[0m\n\u001b[1;32m    130\u001b[0m     ):\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mapply_grad_to_update_var\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/adam.py\u001b[0m in \u001b[0;36mupdate_step\u001b[0;34m(self, gradient, variable, learning_rate)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_velocities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_variable_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta_2_power\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta_1_power\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         self.assign_add(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/ops/numpy.py\u001b[0m in \u001b[0;36msqrt\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   5734\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mSqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbolic_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5735\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5736\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/sparse.py\u001b[0m in \u001b[0;36msparse_wrapper\u001b[0;34m(x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    385\u001b[0m             )\n\u001b[1;32m    386\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msparse_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/numpy.py\u001b[0m in \u001b[0;36msqrt\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   2307\u001b[0m     )\n\u001b[1;32m   2308\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2309\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/weak_tensor_ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mx_arg_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_auto_dtype_conversion_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# Global parameters\n",
        "minibatch_size = 50\n",
        "learning_rate = 0.001\n",
        "beta = 0.1\n",
        "## model 1\n",
        "size_input = 784 # MNIST data input (img shape: 28*28)\n",
        "size_hidden = 64\n",
        "\n",
        "size_output = 10\n",
        "gamma = 1.0\n",
        "\n",
        "class Encoder:\n",
        "    def __init__(self, input_size, encoding_dim):\n",
        "        # Define sizes\n",
        "        self.size_input = input_size\n",
        "        self.size_hidden = 64\n",
        "        self.size_output = encoding_dim\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.w1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden], stddev=0.1), trainable=True, name=\"W1\")\n",
        "        self.b1 = tf.Variable(tf.zeros([self.size_hidden]), name='b1')\n",
        "        self.w2 = tf.Variable(tf.random.normal([self.size_hidden, self.size_output], stddev=0.1), trainable=True, name='W2')\n",
        "        self.b2 = tf.Variable(tf.zeros([self.size_output]), name='encoder_b2')\n",
        "\n",
        "        # Initialize feedback weights\n",
        "        self.E2 = tf.Variable(tf.random.normal([self.size_hidden, self.size_hidden], stddev=1.0), name=\"E2\")\n",
        "        self.E3 = tf.Variable(tf.random.normal([self.size_hidden, self.size_hidden], stddev=1.0), name=\"E3\")\n",
        "\n",
        "    def compute_output(self, x):\n",
        "        #print(f\"w1:, {self.w1.shape}\")\n",
        "        #print(f\"b1:,{self.b1.shape}\")\n",
        "        z1 = tf.matmul(x, self.w1) + self.b1\n",
        "        a1 = tf.nn.relu(z1)\n",
        "        #print(\"[forward] a1 shape (after ReLU):\", a1.shape)\n",
        "\n",
        "        #print(f\"w2:, {self.w2.shape}\")\n",
        "        #print(f\"b2:,{self.b2.shape}\")\n",
        "        z2 = tf.matmul(a1, self.w2) + self.b2\n",
        "        #print(\"[forward] z2 shape:\", z2.shape)\n",
        "        a2 = tf.nn.relu(z2)\n",
        "\n",
        "        return {\n",
        "            'layer1_pre_activation': z1,\n",
        "            'layer1_activation': a1,\n",
        "            'layer2_pre_activation': z2,\n",
        "            'layer2_activation': a2,\n",
        "            'final_encoding': a2\n",
        "        }\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.compute_output(x)['final_encoding']\n",
        "\n",
        "    @property\n",
        "    def variables(self):\n",
        "        return [self.w1,  self.w2,  self.E2, self.E3]\n",
        "\n",
        "class Decoder:\n",
        "    def __init__(self, encoding_dim, output_size):\n",
        "        # Define sizes\n",
        "        self.size_input = encoding_dim\n",
        "        self.size_hidden = 64\n",
        "        self.size_output = output_size\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.w3 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden], stddev=0.1), trainable=True, name=\"W3\")\n",
        "        self.b3 = tf.Variable(tf.zeros([self.size_hidden]), name='b3')\n",
        "        self.w4 = tf.Variable(tf.random.normal([self.size_hidden, self.size_output], stddev=0.1), trainable=True, name='W4')\n",
        "        self.b4 = tf.Variable(tf.zeros([self.size_output]), name='decoder_b4')\n",
        "\n",
        "        self.E4 = tf.Variable(tf.random.normal([self.size_output, self.size_hidden], stddev=1.0), name=\"E4\")\n",
        "\n",
        "    def compute_output(self, encoded):\n",
        "        #print(f\"w3: {self.w3.shape}\")\n",
        "        #print(f\"b3:,{self.b3.shape}\")\n",
        "        z3 = tf.matmul(encoded, self.w3) + self.b3\n",
        "        #print(\"[forward] z3 shape:\", z3.shape)\n",
        "        a3 = tf.nn.relu(z3)\n",
        "        #print(f\"w4:, {self.w4.shape}\")\n",
        "        #print(f\"b4:,{self.b4.shape}\")\n",
        "        z4 = tf.matmul(a3, self.w4) + self.b4\n",
        "        #print(\"[forward] z4 shape:\", z4.shape)\n",
        "        reconstruction = tf.nn.sigmoid(z4)\n",
        "\n",
        "        return {\n",
        "            'layer1_pre_activation': z3,\n",
        "            'layer1_activation': a3,\n",
        "            'layer2_pre_activation': z4,\n",
        "            'final_reconstruction': reconstruction\n",
        "        }\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.compute_output(x)['final_reconstruction']\n",
        "\n",
        "    @property\n",
        "    def variables(self):\n",
        "        return [self.w3, self.w4, self.E4]\n",
        "\n",
        "class Autoencoder:\n",
        "    def __init__(self, input_size, encoding_dim):\n",
        "        self.encoder = Encoder(input_size, encoding_dim)\n",
        "        self.decoder = Decoder(encoding_dim, input_size)\n",
        "        self.optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "        # Initialize storage for intermediate values\n",
        "        self.z1 = None\n",
        "        self.a1 = None\n",
        "        self.z2 = None\n",
        "        self.a2 = None\n",
        "        self.z3 = None\n",
        "        self.a3 = None\n",
        "        self.decoder_outputs = None\n",
        "        self.hhat1 = None  # Will store target representations\n",
        "\n",
        "    def encode_and_decode(self, x):\n",
        "        # Forward pass\n",
        "        encoder_outputs = self.encoder.compute_output(x)\n",
        "        decoder_outputs = self.decoder.compute_output(encoder_outputs['final_encoding'])\n",
        "\n",
        "        # Store activations for LRA updates\n",
        "        self.z1 = encoder_outputs['layer1_pre_activation']\n",
        "        self.a1 = encoder_outputs['layer1_activation']\n",
        "        self.z2 = encoder_outputs['layer2_pre_activation']\n",
        "        self.a2 = encoder_outputs['layer2_activation']\n",
        "        self.z3 = decoder_outputs['layer1_pre_activation']\n",
        "        self.a3 = decoder_outputs['layer1_activation']\n",
        "        self.decoder_outputs = decoder_outputs['final_reconstruction']\n",
        "\n",
        "        return {\n",
        "            'encoder_outputs': encoder_outputs,\n",
        "            'decoder_outputs': decoder_outputs,\n",
        "            'final_reconstruction': decoder_outputs['final_reconstruction']\n",
        "        }\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.encode_and_decode(x)['final_reconstruction']\n",
        "\n",
        "    @property\n",
        "    def variables(self):\n",
        "       return [\n",
        "        self.encoder.w1,\n",
        "        self.encoder.w2,\n",
        "        self.decoder.w3,\n",
        "        self.decoder.w4,\n",
        "        self.encoder.E2,\n",
        "        self.encoder.E3,\n",
        "        self.decoder.E4\n",
        "      ]\n",
        "\n",
        "\n",
        "    def compute_loss(self, x_train):\n",
        "        reconstruction_error = tf.subtract(self.decoder_outputs, x_train)\n",
        "        return tf.reduce_mean(tf.square(reconstruction_error))\n",
        "\n",
        "    def compute_lraupdates(self, x_train):\n",
        "        # Forward pass first\n",
        "        self.encode_and_decode(x_train)\n",
        "\n",
        "        # Compute errors and updates\n",
        "        e4 = tf.subtract(self.decoder_outputs, x_train)  # (50,784)\n",
        "        e4_T = tf.transpose(e4)                          # (784,50)\n",
        "\n",
        "        # Compute d3\n",
        "        d3 = tf.matmul(tf.transpose(self.decoder.E4), e4_T)  # E4: (784,64) → E4^T: (64,784) @ e4_T: (784,50) → d3: (64,50)\n",
        "        d3_b = tf.multiply(d3, beta)                        # (64,50)\n",
        "\n",
        "        # Compute y3_z\n",
        "        z3_T = tf.transpose(self.z3)                         # z3: (50,64) → z3_T: (64,50)\n",
        "        y3_z = tf.nn.tanh(tf.subtract(z3_T, d3_b))           # (64,50)\n",
        "\n",
        "        # Compute e3\n",
        "        e3 = tf.subtract(self.a2, tf.transpose(y3_z))        # a2: (50,64) - y3_z_T: (50,64) → e3: (50,64)\n",
        "\n",
        "        # Compute d2\n",
        "        e3_T = tf.transpose(e3)\n",
        "        #print(e3_T.shape)\n",
        "        #print(self.encoder.E3.shape)\n",
        "        d2 = tf.matmul(self.encoder.E3, e3_T)\n",
        "        # E3: (64,64) @ e3_T: (64,50) → d2: (64,50)\n",
        "        d2_b = tf.multiply(d2, beta)                        # (64,50)\n",
        "\n",
        "        # Compute y2_z\n",
        "        z2_T = tf.transpose(self.z2)                         # z2: (50,64) → z2_T: (64,50)\n",
        "        y2_z = tf.nn.tanh(tf.subtract(z2_T, d2_b))           # (64,50)\n",
        "\n",
        "        # Compute e2\n",
        "        e2 = tf.subtract(self.a2, tf.transpose(y2_z))     # hhat1: (50,64) - y2_z_T: (50,64) → e2: (50,64)\n",
        "\n",
        "        # Compute d1\n",
        "        e2_T = tf.transpose(e2)\n",
        "        #print(e2.shape)# (64,50)\n",
        "        d1 = tf.matmul(self.encoder.E2, e2_T)                # E2: (64,64) @ e2_T: (64,50) → d1: (64,50)\n",
        "        d1_b = tf.multiply(d1, beta)                        # (64,50)\n",
        "\n",
        "        # Compute y1_z\n",
        "        z1_T = tf.transpose(self.z1)                         # z1: (50,64) → z1_T: (64,50)\n",
        "        y1_z = tf.nn.tanh(tf.subtract(z1_T, d1_b))           # (64,50)\n",
        "\n",
        "        # Compute e1\n",
        "        e1 = tf.subtract(self.z1, tf.transpose(y1_z))        # z1: (50,64) - y1_z_T: (50,64) → e1: (50,64)\n",
        "\n",
        "        # Compute weight updates\n",
        "        dW4 = tf.matmul(e4, self.z3, transpose_a=True)      # e4: (50,784) @ z3: (50,64) with transpose_a=True → (784,64)\n",
        "        dW3 = tf.matmul(e3, self.z2, transpose_a=True)      # e3: (50,64) @ z2: (50,64) with transpose_a=True → (64,64)\n",
        "        dW2 = tf.matmul(e2, self.z1, transpose_a=True)      # e2: (50,64) @ z1: (50,64) with transpose_a=True → (64,64)\n",
        "        dW1 = tf.matmul(e1, x_train, transpose_a=True)      # e1: (50,64) @ x_train: (50,784) with transpose_a=True → (64,784)\n",
        "\n",
        "        # Compute eligible traces\n",
        "        dW4_e = tf.multiply(dW4, gamma)\n",
        "\n",
        "        # (784,64)\n",
        "        dW3_e = tf.multiply(dW3, gamma)\n",
        "\n",
        "        dW2_e = tf.multiply(dW2, gamma)\n",
        "\n",
        "\n",
        "        # Combine gradients\n",
        "        grads_w = [\n",
        "            tf.transpose(dW1),  # (784,64) → matches w1: (784,64)\n",
        "            tf.transpose(dW2),  # (64,64) → matches w2: (64,64)\n",
        "            tf.transpose(dW3),  # (64,64) → matches w3: (64,64)\n",
        "            tf.transpose(dW4),  # (64,784) → matches w4: (64,784)\n",
        "            dW2_e,              # (64,64) → matches E2: (64,64)\n",
        "            dW3_e,              # (64,64) → matches E3: (64,64)\n",
        "            dW4_e               # (784,64) → matches E4: (784,64)\n",
        "        ]\n",
        "        #for g, v in zip(grads_w, self.variables):\n",
        "           #print(f\"Variable {v.name} shape: {v.shape}, Gradient shape: {g.shape}\")\n",
        "\n",
        "\n",
        "        # Apply updates\n",
        "        self.optimizer.apply_gradients(zip(grads_w, self.variables))\n",
        "\n",
        "        # Return current loss\n",
        "        return self.compute_loss(x_train)\n",
        "\n",
        "def visualize_reconstructions(autoencoder, x_test, num_images=5):\n",
        "    \"\"\"\n",
        "    1) Pick 'num_images' images from x_test\n",
        "    2) Run them through the autoencoder to get reconstructions\n",
        "    3) Plot original (top row) vs. reconstructed (bottom row)\n",
        "    \"\"\"\n",
        "    x_subset = x_test[:num_images]  # or pick random indices if you want\n",
        "    recons = autoencoder(x_subset)  # autoencoder forward pass -> reconstruction\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    for i in range(num_images):\n",
        "        # Original\n",
        "        ax = plt.subplot(2, num_images, i + 1)\n",
        "        plt.imshow(x_subset[i].reshape(28, 28), cmap='gray')\n",
        "        plt.title(\"Original\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Reconstruction\n",
        "        ax = plt.subplot(2, num_images, i + num_images + 1)\n",
        "        plt.imshow(recons[i].numpy().reshape(28, 28), cmap='gray')\n",
        "        plt.title(\"Reconstructed\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def train_autoencoder():\n",
        "    # Prepare MNIST dataset\n",
        "    (x_train, _), (x_test, _) = mnist.load_data()\n",
        "    x_train = x_train.astype(np.float32) / 255.0\n",
        "    x_test = x_test.astype(np.float32) / 255.0\n",
        "\n",
        "    x_train = x_train.reshape(-1, 784)\n",
        "    x_test = x_test.reshape(-1, 784)\n",
        "\n",
        "    # Initialize autoencoder\n",
        "    input_size = 784  # 28*28\n",
        "    encoding_dim = 64\n",
        "    autoencoder = Autoencoder(input_size, encoding_dim)\n",
        "\n",
        "    # Training loop\n",
        "    epochs = 10\n",
        "    batch_size = minibatch_size\n",
        "    n_batches = len(x_train) // batch_size\n",
        "\n",
        "    # Collect training & validation losses\n",
        "    train_losses_per_epoch = []\n",
        "    val_losses_per_epoch = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0.0\n",
        "\n",
        "        # Shuffle training data each epoch\n",
        "        idx = np.random.permutation(len(x_train))\n",
        "        x_train_shuffled = x_train[idx]\n",
        "\n",
        "        #  Training loop\n",
        "        for batch_idx in range(n_batches):\n",
        "            start_idx = batch_idx * batch_size\n",
        "            end_idx = start_idx + batch_size\n",
        "            batch = x_train_shuffled[start_idx:end_idx]\n",
        "\n",
        "            # LRA update\n",
        "            loss = autoencoder.compute_lraupdates(batch)\n",
        "            total_loss += loss.numpy()\n",
        "\n",
        "            #if batch_idx % 10 == 0:\n",
        "                #print(f\"Epoch {epoch+1}/{epochs}, Batch {batch_idx}/{n_batches}, Batch Loss: {loss:.4f}\")\n",
        "\n",
        "        #  Average training loss for the epoch\n",
        "        avg_train_loss = total_loss / n_batches\n",
        "        train_losses_per_epoch.append(avg_train_loss)\n",
        "\n",
        "        #  Validation (test) loss\n",
        "        # first 1000 test samples for speed\n",
        "        test_subset = 1000\n",
        "        recon_test = autoencoder(x_test[:test_subset])\n",
        "        test_loss = tf.reduce_mean(tf.square(x_test[:test_subset] - recon_test)).numpy()\n",
        "        val_losses_per_epoch.append(test_loss)\n",
        "\n",
        "        # Print results\n",
        "        print(f\"Epoch {epoch+1} completed\")\n",
        "        print(f\"Train Loss: {avg_train_loss:.4f}, Test Loss: {test_loss:.4f}\\n\")\n",
        "\n",
        "    #  Plot the training and validation (test) loss\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.figure(figsize=(8,5))\n",
        "    plt.plot(range(1, epochs+1), train_losses_per_epoch, label='Train Loss')\n",
        "    plt.plot(range(1, epochs+1), val_losses_per_epoch, label='Test Loss')\n",
        "    plt.title(\"Training vs. Test Loss Over Epochs\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    # After training, visualize some reconstructions!\n",
        "    visualize_reconstructions(autoencoder, x_test, num_images=5)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_autoencoder()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "k5dHWOBJkIci",
        "outputId": "95fdd04e-d080-4227-f622-876c71a44cee"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 completed\n",
            "Train Loss: 0.0780, Test Loss: 0.0680\n",
            "\n",
            "Epoch 2 completed\n",
            "Train Loss: 0.0714, Test Loss: 0.0732\n",
            "\n",
            "Epoch 3 completed\n",
            "Train Loss: 0.0718, Test Loss: 0.0695\n",
            "\n",
            "Epoch 4 completed\n",
            "Train Loss: 0.0724, Test Loss: 0.0717\n",
            "\n",
            "Epoch 5 completed\n",
            "Train Loss: 0.0728, Test Loss: 0.0707\n",
            "\n",
            "Epoch 6 completed\n",
            "Train Loss: 0.0733, Test Loss: 0.0716\n",
            "\n",
            "Epoch 7 completed\n",
            "Train Loss: 0.0737, Test Loss: 0.0776\n",
            "\n",
            "Epoch 8 completed\n",
            "Train Loss: 0.0739, Test Loss: 0.0735\n",
            "\n",
            "Epoch 9 completed\n",
            "Train Loss: 0.0745, Test Loss: 0.0740\n",
            "\n",
            "Epoch 10 completed\n",
            "Train Loss: 0.0749, Test Loss: 0.0706\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}